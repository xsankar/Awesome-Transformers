# Awesome-Transformers
#### _[<img src="images/back_button_2.png" width="25" height="25">Back to TOC](https://github.com/xsankar/Awesome-Awesome-LLM)_
| [About Me](https://www.linkedin.com/in/ksankar) | [Blog](https://ksankar.medium.com) |
| :- | :- |
> #### All things Transformers
> |***As of 12.27.23, I am working hard to build the repos - takes time to review and curate. Appreciate your patience ... Thanks ...***|
> | :- |
> 
---
| [Paper Collection](#paper-collection) | [Other Repos](#other-repos) |
| :-: | :-: | 
---
### Paper Collection
[<img src="images/back_button.png" width="25" height="25">Top](#mback-to-toc)
| | Title | Notes |
| :- | :- | :- |
| |https://github.com/Xnhyacinth/Awesome-LLM-Long-Context-Modeling | Efficient Transformers, Length Extrapolation, Long Term Memory, Retrieval Augmented Generation(RAG), and Evaluation for Long Context Modeling |
---
### Other Repos
[<img src="images/back_button.png" width="25" height="25">Top](#mback-to-toc)
| | Title | Notes |
| :- | :- | :- |
| | https://github.com/cmhungsteve/Awesome-Transformer-Attention | |
---
